run_mode : train
print_after_steps : 50
save_interval : !!float 10000
output_dir: "./output"
load_model_from_ckpt: #"./output/checkpoints/dummy.pt"
restore_from_checkpoint_path:
epochs: 1
profile: False
exec:
  model:
    name: TwinBert
    bert: 
      vocab_size : 30522
      hidden_size : 768
      num_hidden_layers : 4
      num_attention_heads : 12
      intermediate_size : 3072
      hidden_act : "gelu"
      hidden_dropout_prob : 0.1
      attention_probs_dropout_prob : 0.1
      max_position_embeddings : 25
      type_vocab_size : 1
      initializer_range : 0.02
      layer_norm_eps : !!float 1e-12
      pad_token_id : 0
      position_embedding_type : "absolute"
      use_cache : True
      classifier_dropout : None
      is_decoder : False
    use_separate_doc_encoder : true
    downscale: 64
    pooler: WeightPooler
    optimizer:
      name: adam
      lr: !!float 1e-4
      weight_decay: 0.01
      warmup_steps: 100
      lr_min_ratio: !!float 1e-4
      lr_schedular: linear
    loss:
      name: Clip
      negative_samples_cnt: 0
      logit_scale: 0.7
      negative_sampling_strategy: 
      undersample_similar_queries: False
      use_one_hot_label: False
    fix_params: #"model_param1,model_param2"
    metrics:
      validation: "accuracy" #"accuracy,prauc"
    compute_embeddings: 
      query: 
        file: 'C:\Users\thumm\Documents\machineLearning\nlp\data\qid_query.tsv'
        bs: 256
      doc: 
        file: 'C:\Users\thumm\Documents\machineLearning\nlp\data\did_doc.tsv'
        bs: 128
      write_to_dir: 'C:\Users\thumm\Documents\machineLearning\nlp\code\pyTorchProject\output\embeddings'
  data:
    train:
      type: qo_dataset
      load_via: npzfilelist
      root: 'C:\\Users\\thumm\\Documents\\machineLearning\\nlp\\data\\orcas_npz_splits'
      file_to_len_mapping: 'filesLenMapping.tsv'
      batch_size: 64
      drop_last: True
      max_seq_len_q: 5
      max_seq_len_doc: 25
      input_schema: "qid:int,query:str,did:int,doc:str"
      useful_cols: "qid,query,did,doc"
      use_segment_id: False
      loader:
        num_cpu_workers: 4
        shuffle: True
        use_pin_memory: True
      sampler:
        name:
        shuffle: True
    validation:
      type: qo_dataset
      load_via: textfilelist
      root: 'C:\Users\thumm\Documents\machineLearning\nlp\data\validation\splits\approx60M_1.tsv'
      batch_size: 64
      drop_last: False
      max_seq_len_q: 5
      max_seq_len_doc: 25
      embedding_type: wordpiece
      input_schema: "qid:str,query:str,did:str,doc:str,label:int"
      useful_cols: "query,doc,label"
      use_segment_id: False
      loader:
        num_cpu_workers: 2
        shuffle: False
        use_pin_memory: True
      sampler:
        name:
        shuffle: True
    # train:
    #   type: qo_dataset
    #   load_via: textfilelist
    #   root: 'C:\\Users\\thumm\\Documents\\machineLearning\\nlp\\data\\orcas_processed.tsv'
    #   file_to_len_mapping:
    #   batch_size: 64
    #   drop_last: True
    #   max_seq_len_q: 5
    #   max_seq_len_doc: 25
    #   input_schema: "qid:str,query:str,did:str,doc:str"
    #   useful_cols: "query,doc"
    #   use_segment_id: False
    #   loader:
    #     num_cpu_workers: 4
    #     shuffle: True
    #     use_pin_memory: True
    #   sampler:
    #     name:
    #     shuffle: True
    # validation:
    #   type: qo_dataset
    #   load_via: textfilelist
    #   root: 'C:\Users\thumm\Documents\machineLearning\nlp\data\validation\splits\approx60M_1.tsv'
    #   batch_size: 64
    #   drop_last: False
    #   max_seq_len_q: 5
    #   max_seq_len_doc: 25
    #   embedding_type: wordpiece
    #   input_schema: "qid:str,query:str,did:str,doc:str,label:int"
    #   useful_cols: "query,doc,label"
    #   use_segment_id: False
    #   loader:
    #     num_cpu_workers: 2
    #     shuffle: False
    #     use_pin_memory: True
    #   sampler:
    #     name:
    #     shuffle: True
    test:
      type: qo_dataset
      load_via: textfilelist
      root: "./validation/textfile.tsv"
      batch_size: 64
      drop_last: False
      max_seq_len_q: 5
      max_seq_len_doc: 25
      embedding_type: wordpiece
      input_schema: "qid:str,query:str,did:str,doc:str"
      useful_cols: "qid,query,did,doc"
      use_segment_id: False